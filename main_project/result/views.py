import os
import random
import re
import time
import requests
import pdfplumber
#import fitz  # PyMuPDF
from pdf2image import convert_from_path, convert_from_bytes
import pytesseract
import io
import markdown 
import tempfile
import shutil 
from pytube import YouTube
import subprocess
from PIL import Image
from django.contrib import messages
from django.http import JsonResponse, StreamingHttpResponse
from django.views.decorators.csrf import csrf_exempt
from django.contrib.auth.decorators import login_required
import json

from django.conf import settings
from django.shortcuts import render, get_object_or_404, redirect
from knowbite.models import UploadedFile, Summary, ChatMessage, ExtractedText, Quiz
from django.utils.safestring import mark_safe
from google import genai
import assemblyai as aai

aai.settings.api_key = settings.ASSEMBLYAI_API_KEY
# Set up Gemini API client  
client = genai.Client(api_key=settings.GEMINI_API_KEY)


# Generation configuration for Gemini
generation_config = {
    "temperature": 0.7,  # Adjust creativity level
    "top_p": 0.9,
    "top_k": 50,
    "max_output_tokens": 8000,
}


SYSTEM_BASE = """You are a helpful teacher assisting students. Follow these rules:
1. Answer using document summary and chat history as context but also add necessary related information if needed.
2. Format math with LaTeX: $inline$ and $$display$$
3. Explain concepts in easy to understand terms and use relevant extra knowledge when helpful
4. Keep some answers under 150 words unless necessary then up to 500 words.
5. Be friendly and use occasional emojis
6. If question is unrelated, politely decline
Current Document Summary: {summary}"""


def extract_text_from_pdf(pdf_path):
    """Extract text from a PDF using PyMuPDF (fitz)."""
    text = ""
    try:
        with pdfplumber.open(pdf_path) as pdf:
            text = ""
            for page in pdf.pages:
                page_text = page.extract_text() or ""
                text += page_text
            
            if len(text.strip()) > 100:
                return text
    except Exception:
        pass
    
    # For PyMuPDF
    try:
        doc = fitz.open(pdf_path)
        text = ""
        for page in doc:
            text += page.get_text()
            
        if len(text.strip()) > 100:
            return text
    except Exception:
        pass
    
    # For OCR
    try:
        images = convert_from_path(pdf_path)
        text = ""
        for image in images:
            text += pytesseract.image_to_string(image, config='--psm 6') + "\n"
            
        return text
    except Exception as e:
        raise Exception(f"Failed to extract text from PDF: {str(e)}")


def extract_text_from_txt(txt_path):
    """Extract text from a TXT file."""
    with open(txt_path, "r", encoding="utf-8") as f:
        return f.read().strip()
    


def generate_summary_with_gemini(text):
    """Send extracted text to Gemini API and get structured summary based on plan type."""

    prompt = f"""
You are an expert educational blogger and content designer. Your job is to turn the following text into a beautifully formatted, engaging blog post summary that will be rendered within a <div> container on a modern website.

IMPORTANT FORMATTING RULES:
- Do NOT include a table of contents or any list of sections.
- All output must be valid HTML that renders beautifully (do NOT use <html>, <body>, or <head> tags).
- **Do not put the output in a div**
- Add style tag with css classes for the summary
- **Do not set a background color**
- Do not set a font family
- Start section headers with black colored <h4> (<h1>, <h2>, or <h3>) and relevant emojis in headers.
- Use semantic HTML5 tags and CSS classes that work with standard blog styling.
- Use short paragraphs, lots of white space, and clear visual separation between sections.
- Use modern, attractive formatting and layout throughout, like top-tier blogs (Medium, Notion, Substack).
- Use css styled <table> for comparisons or data, <blockquote> and <div class='pro-tip'> for callouts, and <mark>, <b>, <i>, <u>, <code>, <span class='highlight'> for emphasis.
- All HTML elements must be properly nested and closed.
- Use $...$ for inline math and formulas (LaTeX style).
- Make the summary at least a 1000 - 3000 words based on the text length
- Make it elaborate and verbose
- Begin with a compelling hook and overview in a <h4> tag, using a carefully chosen emoji
- Use professional, modern section headers in <h4> tags with unique, relevant emojis for each section
- Break down complex concepts into digestible, visually separated sections with short paragraphs and clear subheadings
- **Always** Use <table> for data, comparisons, or frameworks (at least one, make sure it is styled for clarity and visual appeal)
- Include graphs, diagrams, flowchart or any other visual element where necessary
- Add advanced formatting: <blockquote>, <div class='pro-tip'>, <mark>, <b>, <i>, <u>, <code> for code snippets, and <span class='highlight'> for key points
- Always include at least one 'Pro Tip', 'Expert Insight', or 'Did You Know?' in a callout box with a distinct style
- Use bullet points, numbered lists, and callouts for clarity and engagement
- Add practical examples, real-world applications, and actionable advice in visually distinct boxes or highlights
- Use $...$ for inline math and highlight important equations
- Include at least **1-2 example** of step by step solution if it contains any mathematical or programming content.
- End with a strong, visually distinct conclusion in a <h4> tag with an emoji
- Make the layout look like a top-tier, beautiful blog (think Medium, Notion, Substack, or top SaaS blogs)
- Use short paragraphs, white space, and modern web style for maximum readability
Text: {text}
"""

    try:
        
        if not text or len(text.strip()) == 0:
            print("Error: Empty input text")
            return "Error: No text content available to summarize"
            
        response = client.models.generate_content(
            model="gemini-2.0-flash-lite", contents=prompt
        )
        
        if not response:
            print("Error: No response from Gemini API")
            return "Error: Failed to generate summary - no response from AI"
            
        if not response.text:
            print("Error: Empty response text from Gemini API")
            return "Error: Failed to generate summary - empty response"
            
        print(f"Successfully generated summary of length: {len(response.text)}")
        return response.text
        
    except Exception as e:
        print(f"Error in generate_summary_with_gemini: {str(e)}")
        return f"Error: Failed to generate summary - {str(e)}"


def generate_with_retries(text, max_retries=4, initial_delay=2):
    """Call generate_summary_with_gemini with retries on rate-limit (429) or transient errors.

    Returns the summary string, or an error string starting with 'Error:' on non-retryable failures.
    """
    delay = initial_delay
    for attempt in range(1, max_retries + 1):
        result = generate_summary_with_gemini(text)

        # If successful (not an error string), return it
        if not (isinstance(result, str) and result.startswith('Error')):
            return result

        # Inspect the error message to decide whether to retry
        err = result.lower()
        retryable = False

        # Common indicators of rate limit / transient errors
        if '429' in err or 'rate' in err or 'rate limit' in err or 'too many requests' in err or 'quota' in err or 'temporar' in err:
            retryable = True

        if not retryable:
            # Non-retryable error â€” return immediately
            return result

        # If we still have attempts left, sleep and retry
        if attempt < max_retries:
            print(f"Rate limit detected from Gemini, attempt {attempt}/{max_retries}. Backing off {delay}s...")
            time.sleep(delay)
            delay *= 2
            continue
        else:
            print(f"Exceeded retries ({max_retries}) for Gemini rate limit.")
            return result


# If needed, support long text by summarizing in chunks.
def split_text(text, max_chars=3000):
    return [text[i:i+max_chars] for i in range(0, len(text), max_chars)]



def generate_long_summary(text):
    chunks = split_text(text, max_chars=3000)
    chunk_summaries = []
    for chunk in chunks:
        summary_chunk = generate_summary_with_gemini(chunk)
        chunk_summaries.append(summary_chunk)
        time.sleep(1)  # Delay to avoid rate limits
    combined_text = " ".join(chunk_summaries)
    final_summary = generate_summary_with_gemini(combined_text)
    return final_summary



def base(request, file_id):
    """Base view for the main page."""
    file = UploadedFile.objects.filter(user=request.user)

    return render(request, "result/base_result.html", {"file": file})

@login_required
def summary_result(request, file_id):
    """Handle document summary and chat interactions"""
    uploaded_file = get_object_or_404(UploadedFile, id=file_id, user=request.user)    # Handle "regenerate" requests    
    if "regenerate" in request.GET:
        # Check regeneration limits
        print('Start regeneration check')
        try:
            user_subscription = request.user.usersubscription
            print(f"User Plan: {user_subscription.plan.name}")
            print(f"Plan regenerations allowed: {user_subscription.plan.summary_regenerations_per_file}")
            
            # Get all summaries for this file
            summaries = Summary.objects.filter(
                user=request.user,
                uploaded_file_id=file_id
            ).order_by('created_at')
            print(f"Total summaries for this file: {summaries.count()}")
            
            if summaries.exists():
                initial_summary = summaries.first()
                regenerations = summaries.filter(created_at__gt=initial_summary.created_at).count()
                print(f"Number of regenerations: {regenerations}")
            
            can_regenerate, message = user_subscription.can_regenerate_summary(file_id)
            print(f"Can regenerate: {can_regenerate}")
            print(f"Message: {message}")
            
            if not can_regenerate:
                if request.headers.get('X-Requested-With') == 'XMLHttpRequest':
                    return JsonResponse({'error': message}, status=403)
                messages.error(request, message)
                return redirect('summary', file_id=file_id)
        except Exception as e:
            messages.error(request, "Error checking subscription limits")
            return redirect('summary', file_id=file_id)

        try:     
            print('start')       
            summary = generate_or_retrieve_summary(request, uploaded_file)
            print('done')
            
            if request.headers.get('X-Requested-With') == 'XMLHttpRequest':
                if isinstance(summary, str) and summary.startswith('Error'):
                    # If we got an error string back
                    print(f"Error during summary generation: {summary}")
                    return JsonResponse({'error': summary}, status=500)
                    
                return JsonResponse({
                    'summary': summary,
                    'success': True,
                    'timestamp': time.time()
                })
                
            return render(request, 'result/summary.html', {
                'summary': summary,
                'file': uploaded_file,
                'chat_history': []  # Reset chat history for regenerated summary
            })
            
        except Exception as e:
            print(f"Error during summary regeneration: {str(e)}")
            if request.headers.get('X-Requested-With') == 'XMLHttpRequest':
                return JsonResponse({
                    'error': f"Failed to regenerate summary: {str(e)}"
                }, status=500)
            messages.error(request, f"Failed to regenerate summary: {str(e)}")
            return redirect('summary', file_id=file_id)

    # Handle normal page load
    summary_instance = Summary.objects.filter(user=request.user, uploaded_file=uploaded_file).first()
    if summary_instance:
        summary = summary_instance.summary_text
    else:
        try:
            summary = generate_or_retrieve_summary(request, uploaded_file)
        except Exception as e:
            summary = f"Error generating summary: {str(e)}"

    formatted_summary = markdown.markdown(summary) if summary else "No summary available"

    # Handle chat message via AJAX
    if request.method == 'POST' and request.headers.get('X-Requested-With') == 'XMLHttpRequest':
        return handle_chat_request(request, uploaded_file)

    # Chat history display
    chat_history = ChatMessage.objects.filter(
        user=request.user,
        file=uploaded_file
    ).order_by('-timestamp')[:10][::-1]

    return render(request, "result/summary.html", {
        "file": uploaded_file,
        "summary": formatted_summary,
        "chat_history": [
            {
                **msg.__dict__,
                'formatted_content': markdown.markdown(msg.content)
            }
            for msg in chat_history
        ]
    })



def generate_or_retrieve_summary(request, uploaded_file):
    """Generate or retrieve document summary"""
    user = request.user  # Force evaluation of lazy object
    is_youtube = uploaded_file.file_type == 'youtube'
    summary_instance = Summary.objects.filter(user=user, uploaded_file=uploaded_file).first()
    extracted_text_instance = ExtractedText.objects.filter(user=user, uploaded_file=uploaded_file).first()
    extracted_text = ""

    if extracted_text_instance is not None:
        extracted_text = extracted_text_instance.extracted_text
    else:
        try:
            # Text extraction step
            if is_youtube:
                print("Extracting YouTube transcript")
            else:
                file_path = uploaded_file.file.path
                try:
                    if file_path.lower().endswith(".pdf"):
                        extracted_text = extract_text_from_pdf(file_path)
                    elif file_path.lower().endswith(".txt"):
                        extracted_text = extract_text_from_txt(file_path)
                    elif file_path.lower().endswith((".wav", ".mp3", ".ogg", ".m4a")):
                        extracted_text = transcribe_audio_assemblyai(file_path)
                except Exception as e:
                    print(f"Text extraction error: {e}")
                    return f"Error in text extraction: {str(e)}"
                    
            # Create extracted text record
            try:
                ExtractedText.objects.create(user=user, uploaded_file=uploaded_file, extracted_text=extracted_text)
            except Exception as e:
                print(f"ExtractedText creation error: {e}")
                return f"Error creating extracted text record: {str(e)}"
                
        except Exception as e:
            print(f"General extraction error: {e}")
            return f"Error during processing: {str(e)}"    # Summary generation step
    try:
        # Get user's plan type
        try:
            user_subscription = user.usersubscription
            plan_type = user_subscription.plan.name
            print(f"User plan type: {plan_type}")
        except Exception as e:
            print(f"Error getting plan type: {e}")          # Define chunk size based on plan type
       
        chunk_size = 10000
        if len(extracted_text) > chunk_size:
            print(f"Text length {len(extracted_text)} exceeds {chunk_size} chars, splitting into chunks")
            chunks = split_text(extracted_text, max_chars=chunk_size)
            chunk_summaries = []
            
            # Calculate word limit per chunk based on total chunks
            word_limits = {
                'free': 500,
                'basic': 700,
                'pro': 1000
            }
            total_word_limit = word_limits.get(plan_type.lower(), 300)
            chunk_word_limit = total_word_limit // len(chunks)
            
            for i, chunk in enumerate(chunks, 1):
                print(f"Processing chunk {i} of {len(chunks)}")
                summary_chunk = generate_with_retries(chunk, max_retries=4, initial_delay=2)

                if isinstance(summary_chunk, str) and summary_chunk.startswith('Error:'):
                    print(f"Error in chunk {i}: {summary_chunk}")
                    return summary_chunk

                chunk_summaries.append(summary_chunk)
                time.sleep(1)  # Delay to avoid rate limits
            
            print(f"Generated {len(chunk_summaries)} chunk summaries")
            
            chunk_summaries = ' '.join(chunk_summaries)
            combined_summary = generate_with_retries(chunk_summaries, max_retries=4, initial_delay=2)
                
            if isinstance(combined_summary, str) and combined_summary.startswith('Error:'):
                print(f"Error combining summaries: {combined_summary}")
                return combined_summary
                
            summary = combined_summary
        else:
            summary = generate_with_retries(extracted_text, max_retries=4, initial_delay=2)
    except Exception as e:
        print(f"Summary generation error: {e}")
        return f"Error generating summary: {str(e)}"
        
    # Save summary
    try:
        if summary_instance:
            summary_instance.summary_text = summary
            summary_instance.save()
        else:
            Summary.objects.create(user=user, uploaded_file=uploaded_file, summary_text=summary)
    except Exception as e:
        print(f"Summary saving error: {e}")
        return f"Error saving summary: {str(e)}"
    
    return summary


@login_required
def handle_chat_request(request, uploaded_file):
    """Process chat messages with Gemini"""    # Check chat message limits
    try:
        user_subscription = request.user.usersubscription
        can_chat, message = user_subscription.can_send_chat_message(uploaded_file.id)
        if not can_chat:
            return JsonResponse({'error': message}, status=403)
    except Exception as e:
        return JsonResponse({'error': 'Error checking chat limits'}, status=500)

    if request.POST.get('message') != None:
        user_message = request.POST.get('message').strip()
    else:
        user_message = request.POST.get('message-chat').strip()

    if not user_message:
        return JsonResponse({'error': 'Empty message'}, status=400)

    # Get document summary once
    summary_text = Summary.objects.filter(
        user=request.user, 
        uploaded_file=uploaded_file
    ).values_list('summary_text', flat=True).first() or "No summary available"

    # Create system instruction with summary
    system_instruction = SYSTEM_BASE.format(summary=summary_text)

    # Get last 3 exchanges (6 messages)
    history_messages = ChatMessage.objects.filter(
        user=request.user,
        file=uploaded_file
    ).order_by('-timestamp')[:6]

    # Format history for Gemini
    history = []
    for msg in reversed(history_messages):
        history.append({
            "role": "user" if msg.role == "user" else "model",
            "parts": [msg.content]
        })

    try:
        # Build a single prompt: system instruction + recent chat history + current user message
        history_lines = []
        for msg in reversed(history_messages):
            role = 'User' if msg.role == 'user' else 'Assistant'
            # Keep content short for prompt, but preserve formatting
            history_lines.append(f"{role}: {msg.content}")

        prompt_parts = [system_instruction]
        if history_lines:
            prompt_parts.append("Conversation history:")
            prompt_parts.extend(history_lines)

        prompt_parts.append(f"User: {user_message}")
        full_prompt = "\n\n".join(prompt_parts)

        # Persist the user's message to chat history
        ChatMessage.objects.create(
            user=request.user,
            file=uploaded_file,
            role='user',
            content=user_message
        )

        # Create a chat using the new genai client API and send the prompt
        chat = client.chats.create(model='gemini-2.0-flash')
        response = chat.send_message(message=full_prompt)

        # Try common response attributes
        bot_response = None
        if hasattr(response, 'text') and response.text:
            bot_response = response.text
        elif hasattr(response, 'response') and response.response:
            bot_response = response.response
        else:
            # Fallback to string representation
            bot_response = str(response)

        # Store bot response
        ChatMessage.objects.create(
            user=request.user,
            file=uploaded_file,
            role='bot',
            content=bot_response
        )

        return JsonResponse({'response': bot_response})

    except Exception as e:
        print(f"Chat error: {str(e)}")
        return JsonResponse({'error': str(e)}, status=500)


def generate_mcqs_with_gemini(summary_text, num_questions, difficulty):
    """Generate multiple-choice questions dynamically based on the summary."""
    
    prompt = f"""
    Generate {num_questions} multiple-choice questions based on the following summary. 
    The questions should be {difficulty} level.
    Use Latex formulas **where necessary**.
    Use diagrams such as graphs, circuit diagrams etc **if necessary**.
    You can ask questions using related images using the html image tag.

    - Each question must have four answer choices (A, B, C, D).
    - Clearly mark the correct answer.
    - The format should be:
      
      Question: ...
      A) ...
      B) ...
      C) ...
      D) ...
      Correct Answer: X
    
    Summary: {summary_text}
    """
    response = client.models.generate_content(
        model="gemini-2.0-flash-lite", contents=prompt
    )
    return response.text if response.text else "No MCQs generated."


def parse_mcq_response(mcq_text):
    """Extract MCQs from AI-generated response."""
    mcqs = []
    questions = mcq_text.split("Question: ")[1:]  # Split based on "Question: "

    for q in questions:
        lines = q.strip().split("\n")
        if len(lines) >= 6:
            question = lines[0]
            option_a = lines[1][3:].strip()  # Remove "A) "
            option_b = lines[2][3:].strip()  # Remove "B) "
            option_c = lines[3][3:].strip()  # Remove "C) "
            option_d = lines[4][3:].strip()  # Remove "D) "
            correct_option = lines[5][-1].strip()  # Last character of "Correct Answer: X"

            mcqs.append({
                "question": question,
                "option_a": option_a,
                "option_b": option_b,
                "option_c": option_c,
                "option_d": option_d,
                "correct_option": correct_option,
            })
    return mcqs

@login_required
def quiz_options(request, file_id):
    uploaded_file = get_object_or_404(UploadedFile, id=file_id, user=request.user)

    return render(request, "result/quiz_options.html", {"file": uploaded_file})

@login_required
def take_quiz(request, file_id):
    """Generates and displays the quiz."""
    uploaded_file = get_object_or_404(UploadedFile, id=file_id, user=request.user)
    
    # Check quiz generation limits
    try:
        user_subscription = request.user.usersubscription
        can_generate, message = user_subscription.can_generate_quiz(file_id)
        if not can_generate:
            print('Cannot generate quiz')
            messages.error(request, message)
            return redirect('quiz_options', file_id=file_id)
    except Exception as e:
        print('Got error')
        messages.error(request, "Error checking quiz limits")
        return redirect('quiz_options', file_id=file_id)
    
    summary_instance = get_object_or_404(Summary, uploaded_file=uploaded_file)

    num_questions = int(request.GET.get("num_questions", 10))
    print(num_questions)
    if request.GET.get("difficulty") == '1':
        difficulty = "easy"
    elif request.GET.get("difficulty") == '3':
        difficulty = "hard"
    else:
        difficulty = "medium"

    print(difficulty)

    # Generate MCQs on the fly
    mcq_text = generate_mcqs_with_gemini(summary_instance.summary_text, num_questions, difficulty)
    mcqs = parse_mcq_response(mcq_text)
    random.shuffle(mcqs)

    Quiz.objects.create(user=request.user, file=uploaded_file)

    request.session['mcqs'] = mcqs  # Store MCQs in session for later use

    return render(request, "result/quiz.html", {"mcqs": mcqs, "file": uploaded_file})

@login_required
def submit_quiz(request, file_id):
    """Handles quiz submission and calculates the score."""
    uploaded_file = get_object_or_404(UploadedFile, id=file_id, user=request.user)
    mcqs = request.session.get("mcqs", [])  # Retrieve MCQs from session

    user_answers = request.POST
    correct_count = 0
    results = []


    for index, mcq in enumerate(mcqs):
        user_choice = user_answers.get(str(index), "")
        option_mapping = {
            "A": mcq["option_a"],
            "B": mcq["option_b"],
            "C": mcq["option_c"],
            "D": mcq["option_d"],
        }
        user_choice_text = option_mapping.get(user_choice, "")
        correct_answer_text = option_mapping.get(mcq["correct_option"], "")
        is_correct = user_choice == mcq["correct_option"]
        if is_correct:
            correct_count += 1
        results.append({
            "question": mcq["question"],
            "user_choice": user_choice_text,
            "correct_choice": correct_answer_text,
            "is_correct": is_correct,
            "options": [mcq["option_a"], mcq["option_b"], mcq["option_c"], mcq["option_d"]],
        })

    score = (correct_count / len(mcqs)) * 100 if mcqs else 0
    incorrect_count = len(mcqs) - correct_count

    return render(request, "result/quiz_result.html", {"mcqs":mcqs, 
                                                        "results": results, 
                                                        "score": score, 
                                                        "file": uploaded_file,
                                                        "correct_count": correct_count,
                                                        "incorrect_count": incorrect_count,})

@login_required
def chatbot(request, file_id):
    uploaded_file = get_object_or_404(UploadedFile, id=file_id, user=request.user)
    summary_instance = Summary.objects.filter(user=request.user, uploaded_file=uploaded_file).first()

    summary = summary_instance.summary_text

    if request.method == 'POST' and request.headers.get('X-Requested-With') == 'XMLHttpRequest':
        return handle_chat_request(request, uploaded_file, summary)
    
    # Display page with history
    chat_history = ChatMessage.objects.filter(
        user=request.user, 
        file=uploaded_file
    ).order_by('-timestamp')[:10][::-1]

    for message in chat_history:
        message.formatted_content = markdown.markdown(message.content)

    context = {
        "file": uploaded_file,
        "chat_history": chat_history
    }
    return render(request, "result/chatbot.html", context)



def transcribe_audio_assemblyai(audio_file_path):
    """Transcribes audio from a file path using AssemblyAI."""
    headers = {"authorization": settings.ASSEMBLYAI_API_KEY}

    try:
        # Upload audio file
        with open(audio_file_path, "rb") as f:
            upload_response = requests.post(
                "https://api.assemblyai.com/v2/upload",
                headers=headers,
                data=f,
                timeout=300
            )
        upload_response.raise_for_status()
        audio_url = upload_response.json()["upload_url"]

        # Start transcription job
        transcript_response = requests.post(
            "https://api.assemblyai.com/v2/transcript",
            headers=headers,
            json={"audio_url": audio_url}
        )
        transcript_response.raise_for_status()
        transcript_id = transcript_response.json()["id"]

        # Poll for result
        polling_url = f"https://api.assemblyai.com/v2/transcript/{transcript_id}"
        while True:
            polling_response = requests.get(polling_url, headers=headers)
            result = polling_response.json()

            if result["status"] == "completed":
                return result["text"]
            elif result["status"] == "error":
                return f"Transcription failed: {result['error']}"

            time.sleep(5)

    except requests.exceptions.RequestException as e:
        return f"Transcription error: {e}"



def download_and_transcribe_youtube(video_url):
    """
    Fetches the transcript from the API and returns ONLY the full text string.
    Returns None if an error occurs.
    """
    API_KEY = os.getenv('TRANSCRIPTAPI_KEY')
    url = 'https://transcriptapi.com/api/v2/youtube/transcript'
    params = {'video_url': video_url, 'format': 'json'}
    
    try:
        response = requests.get(
            url, 
            params=params, 
            headers={'Authorization': 'Bearer ' + API_KEY}, 
            timeout=30
        )
        response.raise_for_status()
        
        data = response.json()
        transcript_segments = data.get('transcript', [])
        
        if not transcript_segments:
            return None

        # Join all segments into one continuous string
        full_text = " ".join([item.get('text', '') for item in transcript_segments])
        
        return full_text

    except Exception as e:
        print(f"Error fetching transcript: {e}")
        return None

@login_required
def transcripts(request, file_id):
    """Display the transcript of the uploaded audio file."""
    uploaded_file = get_object_or_404(UploadedFile, id=file_id, user=request.user)
    transcript_instance = ExtractedText.objects.filter(user=request.user, uploaded_file=uploaded_file).first()
    
    if not transcript_instance:
        return JsonResponse({'error': 'Transcript not found'}, status=404)

    transcript_text = transcript_instance.extracted_text
    formatted_transcript = markdown.markdown(transcript_text) if transcript_text else "No transcript available"
    
    return render(request, "result/extracted_text.html", {
        "file": uploaded_file,
        "transcript": formatted_transcript
    })